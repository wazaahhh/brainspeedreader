\section{Method}
Our standardized experiment involved XXX participants (XX females, XX males) with age ranging from XX to XX years old. For this study, each participant was asked to read 4 newspaper articles (from XX to XX words), randomly selected out of 6. The texts were displayed through rapid serial visual presentation (RSVP) \cite{}, 

Brain activity was measured on purpose with the cheapest consumer grade EEG headset available on the market (Neurosky Mindwave), and Electroencephalogram (EEG) signal was collected on the left forehead (Fp9 position in the 10-20 system). And at each new word presented, a measure of {\it brain activity} was computed out of the last 512 voltage measures (i.e., one second of EEG data at a 512 Hz sampling rate). The {\it brain activity} is defined as the entropy of the power spectrum of the signal,






at a rate of 125 milliseconds (ms) per word. For each article presented, in order to keep some coherence with the rhythm of the text\cite{}, additional time was given for punctuation: 125 x 0.5 = 75 ms for comma and dash, and 125 x 2 = 250 ms for period, semi-column and column symbols, as well as before new paragraphs (125 x 3 = 375 ms).\\


\subsection{Measure of Brain Activity}
While the articles were presented, a consumer grade EEG headset (Neurosky Mindwave) was used to collect Electroencephalogram (EEG) signal on the left forehead (Fp9 position in the 10-20 system). And at each new word presented, a measure of {\it brain activity} was computed out of the last 512 voltage measures (i.e., one second of EEG data at a 512 Hz sampling rate). The {\it brain activity} is defined as the entropy of the power spectrum of the signal,

\begin{equation}
\label{eq:tsallis}
S_q(X) = \frac{1}{q-1} \left[ 1 - \sum_{i=1}^n (p_i)^q \right],
\end{equation}

with $q=1$, which reduces to the Shannon entropy,

\begin{equation}
\label{eq:shannon}
S = - \sum_{i=1}^n p_i\cdot log_{2}(p_i),
\end{equation}

which is well-known in information theory \cite{}. The Shannon entropy is used here as a powerful method to compress the information contained in the power spectrum (i.e., a vector with 512 values) into a scalar. However,  the entropy may also be considered as a very rough measure of disorder in the recorded EEG signal, even though no assumption is made here on how the brain circuitry influences the modulations of EEG waves.\\

Here, we consider the brain as a {\it black box}, which delivers a signal when the participant is subjected to a coherent source of sequential stimuli (i.e., words of an newspaper presented one at the time in a sequential order). Our goal is to identify the text read from the output provided by {\it brain black box}.

\subsection{Decoding Procedure}
We have to make assumptions (small, large words + punctuation?) and tackle high noise.


\subsubsection{Direct Decoding}

\begin{equation}
Entropy~Sequence \rightarrow Text~Sequence
\end{equation}

\subsubsection{Reverse Engineering}

\begin{equation}
Text~Sequences \rightarrow Templates~Entropy~Sequence \Longleftrightarrow Entropy~Sequence  
\end{equation}

\subsection{Evaluation of text pattern uniqueness}

Let's consider two texts defined by their unique sequence of words. The distribution of word lengths (Figure \ref{fig:wl_distribution}a) shows not significant difference between texts. 

However, we notice that small words (with one symbol) and words with more than 9 symbols are both rare.

We hypothesize that these {\it rare} words are uniquely distributed in a sequence of words.


To account for the uniqueness of a text

\begin{equation}
Seq_{words}(N) = \{w_{1},w_{2},...,w_{N}\}
\end{equation}


\begin{equation}
p := pdf(l_{word})
\end{equation}

\begin{equation}
Seq_{1/p}(N) = \left( \frac{1}{p(w_{1})},\frac{1}{p(w_{2})},...,\frac{1}{p(w_{N})} \right)
\end{equation}

Measure of text similarity:

\begin{equation*}
\text{similarity}(Seq_{i}, Seq_{j}) = {Seq_{i} \cdot Seq_{j} \over \|Seq_{i}\|\|Seq_{j}\|}, 
\end{equation*}

As shown on Figure \ref{fig:wl_distribution}b, similarity decreases rapidly (as a power law function or at least exponential) as a function of $N$. For $N > 30$, $avg(similarity) < 0.5$, For $N > 100$, $avg(similarity) < 0.1$. One to one similarities between texts are shown on Figure {\bf \ref{fig:wl_distribution}c}.



